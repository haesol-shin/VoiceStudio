{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81619054298cb84",
   "metadata": {},
   "source": [
    "# Interspeech 2026"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9678cda37dd5de34",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff1a1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from transformers.utils.notebook import NotebookProgressBar\n",
    "\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from voicestudio.utils.audio_utils import show_waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df4c9ee916b8505",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6a9a659920805ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 20 01:31:14 2026       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 580.126.09             Driver Version: 580.126.09     CUDA Version: 13.0     |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX PRO 6000 Blac...    Off |   00000000:01:00.0 Off |                    0 |\n",
      "| 30%   25C    P8             12W /  300W |      18MiB /  97887MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc7cad9ff7ebd6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Using device - cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Set CUDA Device Number\n",
    "DEVICE_NUM = 0\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(f\"cuda:{DEVICE_NUM}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    DEVICE_NUM = -1\n",
    "\n",
    "device_map = f\"cuda:{DEVICE_NUM}\" if DEVICE_NUM >= 0 else \"cpu\"\n",
    "print(f\"INFO: Using device - {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca176a8586463d",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20f3f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spk_incon.datasets import LIBRITTS_P_Custom\n",
    "from spk_incon.datasets.libritts_p3 import download_libritts_p_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27465f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data\"\n",
    "Z_THRESHOLD = 2.5\n",
    "URL = \"https://dolab-data.duckdns.org/api/public/dl/-qA96ilN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dffc1c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(os.path.join(DATA_ROOT, \"train-clean-100.tar.gz\")):\n",
    "    !wget -O \"./data/train-clean-100.tar.gz\" {URL}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9c6e3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading cached dataset from data/.cache/libritts_p_train-clean-100/dataset...\n",
      "[INFO] Filtering outliers (max_z_score=3.5)...\n",
      "[INFO] Filtered: 33187 -> 31504 samples.\n"
     ]
    }
   ],
   "source": [
    "download_libritts_p_metadata(root=DATA_ROOT, annotator=\"df1\")\n",
    "curated_dataset = LIBRITTS_P_Custom(root=DATA_ROOT, download=True, max_z_score=3.5)#float(\"inf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36a00b89ab989e",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45874f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "\n",
    "from voicestudio.models.parler_tts import ParlerTTSForConditionalGeneration\n",
    "from voicestudio.models.qwen3_tts import Qwen3TTSForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "936726a74275f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spk_incon.models.selective_tuner import SelectiveTunerForConditionalGeneration, SelectiveTunerConfig\n",
    "from spk_incon.components.style_anchor import DirectStyleAnchorEmbedding, EncoderStyleAnchorEmbedding, MixedStyleAnchorEmbedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e378ac9",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2433e377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model select\n",
    "#model_id = \"parler-tts/parler-tts-mini-v1\"\n",
    "#model_id = \"parler-tts/parler-tts-large-v1\"\n",
    "#model_id = \"parler-tts/parler-tts-mini-v1.1\"\n",
    "\n",
    "#model_id = \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\"\n",
    "model_id = \"Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09bf7226",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2 without specifying a torch dtype. This might lead to unexpected behaviour\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85147190e6e44f49ce6fe5e07254de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Qwen3TTSForConditionalGeneration(\n",
       "  (talker): Qwen3TTSTalkerForConditionalGeneration(\n",
       "    (model): Qwen3TTSTalkerModel(\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen3TTSTalkerDecoderLayer(\n",
       "          (self_attn): Qwen3TTSTalkerAttention(\n",
       "            (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "            (q_norm): Qwen3TTSRMSNorm((128,), eps=1e-06)\n",
       "            (k_norm): Qwen3TTSRMSNorm((128,), eps=1e-06)\n",
       "          )\n",
       "          (mlp): Qwen3TTSTalkerTextMLP(\n",
       "            (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "            (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n",
       "            (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n",
       "            (act_fn): SiLUActivation()\n",
       "          )\n",
       "          (input_layernorm): Qwen3TTSRMSNorm((2048,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen3TTSRMSNorm((2048,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen3TTSRMSNorm((2048,), eps=1e-06)\n",
       "      (rotary_emb): Qwen3TTSTalkerRotaryEmbedding()\n",
       "      (codec_embedding): Embedding(3072, 2048)\n",
       "      (text_embedding): Embedding(151936, 2048)\n",
       "    )\n",
       "    (text_projection): Qwen3TTSTalkerResizeMLP(\n",
       "      (linear_fc1): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (linear_fc2): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "      (act_fn): SiLUActivation()\n",
       "    )\n",
       "    (codec_head): Linear(in_features=2048, out_features=3072, bias=False)\n",
       "    (code_predictor): Qwen3TTSTalkerCodePredictorModelForConditionalGeneration(\n",
       "      (model): Qwen3TTSTalkerCodePredictorModel(\n",
       "        (layers): ModuleList(\n",
       "          (0-4): 5 x Qwen3TTSDecoderLayer(\n",
       "            (self_attn): Qwen3TTSAttention(\n",
       "              (q_proj): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (o_proj): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "              (q_norm): Qwen3TTSRMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3TTSRMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3TTSTalkerTextMLP(\n",
       "              (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "              (act_fn): SiLUActivation()\n",
       "            )\n",
       "            (input_layernorm): Qwen3TTSRMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3TTSRMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3TTSRMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3TTSRotaryEmbedding()\n",
       "        (codec_embedding): ModuleList(\n",
       "          (0-14): 15 x Embedding(2048, 2048)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): ModuleList(\n",
       "        (0-14): 15 x Linear(in_features=1024, out_features=2048, bias=False)\n",
       "      )\n",
       "      (small_to_mtp_projection): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model loading\n",
    "if \"parler\" in model_id.lower():\n",
    "    model = ParlerTTSForConditionalGeneration.from_pretrained(\n",
    "        model_id, device_map=device_map\n",
    "    )\n",
    "    model_dtype = model.config.dtype = model.dtype\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "elif \"qwen\" in model_id.lower():\n",
    "    model = Qwen3TTSForConditionalGeneration.from_pretrained(\n",
    "        model_id, device_map=device_map, dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\",\n",
    "    )\n",
    "    model_dtype = model.config.dtype = model.dtype\n",
    "    processor = AutoProcessor.from_pretrained(model_id, device_map=device_map)\n",
    "    tokenizer = processor.tokenizer\n",
    "else:\n",
    "    pass\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647c3f2d",
   "metadata": {},
   "source": [
    "### Embedding-tuner Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246205fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode settings\n",
    "ENABLE_BOS_TOKEN_TUNING = False  # this wiil trigger use_mixed_anchor\n",
    "ADD_CONSISTENCY_TOKEN = True\n",
    "ADD_STYLE_TOKEN = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6400dfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token definitions\n",
    "BOS_TOKEN = \"</s>\"\n",
    "BOS_TOKEN_ID = 1\n",
    "STYLE_TOKEN = \"<style>\"\n",
    "STYLE_TOKEN_ID = len(tokenizer)\n",
    "CONSISTENCY_TOKEN = \"<consistency>\"\n",
    "CONSISTENCY_TOKEN_ID = len(tokenizer) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1202d392",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_token = []\n",
    "anchor_token_id = []\n",
    "use_direct_anchor = False\n",
    "use_mixed_anchor = False\n",
    "result_id = \"\"\n",
    "\n",
    "if ADD_CONSISTENCY_TOKEN:\n",
    "    anchor_token.append(CONSISTENCY_TOKEN)\n",
    "    anchor_token_id.append(CONSISTENCY_TOKEN_ID)\n",
    "    result_id = \"consistency\"\n",
    "if ADD_STYLE_TOKEN:\n",
    "    anchor_token.append(STYLE_TOKEN)\n",
    "    anchor_token_id.append(STYLE_TOKEN_ID)\n",
    "    if result_id:\n",
    "        result_id = \"all\"\n",
    "    else:\n",
    "        result_id = \"style\"\n",
    "\n",
    "anchor_token = tuple(anchor_token)\n",
    "anchor_token_id = tuple(anchor_token_id)\n",
    "\n",
    "if ENABLE_BOS_TOKEN_TUNING:\n",
    "    use_mixed_anchor = True\n",
    "    anchor_token = ((BOS_TOKEN, ), anchor_token)\n",
    "    anchor_token_id = ((BOS_TOKEN_ID, ), anchor_token_id)\n",
    "    result_id += \"_bos\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f132c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Model Config:\n",
      "return_dict: True\n",
      "output_hidden_states: False\n",
      "torchscript: False\n",
      "dtype: bfloat16\n",
      "pruned_heads: {}\n",
      "tie_word_embeddings: True\n",
      "chunk_size_feed_forward: 0\n",
      "is_encoder_decoder: False\n",
      "is_decoder: False\n",
      "cross_attention_hidden_size: None\n",
      "add_cross_attention: False\n",
      "tie_encoder_decoder: False\n",
      "architectures: ['Qwen3TTSForConditionalGeneration']\n",
      "finetuning_task: None\n",
      "id2label: {0: 'LABEL_0', 1: 'LABEL_1'}\n",
      "label2id: {'LABEL_0': 0, 'LABEL_1': 1}\n",
      "task_specific_params: None\n",
      "problem_type: None\n",
      "tokenizer_class: None\n",
      "prefix: None\n",
      "bos_token_id: None\n",
      "pad_token_id: None\n",
      "eos_token_id: None\n",
      "sep_token_id: None\n",
      "decoder_start_token_id: None\n",
      "max_length: 20\n",
      "min_length: 0\n",
      "do_sample: False\n",
      "early_stopping: False\n",
      "num_beams: 1\n",
      "temperature: 1.0\n",
      "top_k: 50\n",
      "top_p: 1.0\n",
      "typical_p: 1.0\n",
      "repetition_penalty: 1.0\n",
      "length_penalty: 1.0\n",
      "no_repeat_ngram_size: 0\n",
      "encoder_no_repeat_ngram_size: 0\n",
      "bad_words_ids: None\n",
      "num_return_sequences: 1\n",
      "output_scores: False\n",
      "return_dict_in_generate: False\n",
      "forced_bos_token_id: None\n",
      "forced_eos_token_id: None\n",
      "remove_invalid_values: False\n",
      "exponential_decay_length_penalty: None\n",
      "suppress_tokens: None\n",
      "begin_suppress_tokens: None\n",
      "num_beam_groups: 1\n",
      "diversity_penalty: 0.0\n",
      "_name_or_path: Qwen/Qwen3-TTS-12Hz-1.7B-VoiceDesign\n",
      "transformers_version: 4.57.6\n",
      "assistant_token_id: 77091\n",
      "model_type: qwen3_tts\n",
      "tf_legacy_loss: False\n",
      "use_bfloat16: False\n",
      "talker_config: {'return_dict': True, 'output_hidden_states': False, 'torchscript': False, 'dtype': 'bfloat16', 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'task_specific_params': None, 'problem_type': None, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'num_beam_groups': 1, 'diversity_penalty': 0.0, '_name_or_path': '', 'head_dim': 128, 'model_type': 'qwen3_tts_talker', 'position_id_per_seconds': 13, 'text_vocab_size': 151936, 'tf_legacy_loss': False, 'use_bfloat16': False, 'vocab_size': 3072, 'max_position_embeddings': 32768, 'hidden_size': 2048, 'intermediate_size': 6144, 'num_hidden_layers': 28, 'num_attention_heads': 16, 'use_sliding_window': False, 'sliding_window': None, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-06, 'use_cache': True, 'rope_theta': 1000000, 'rope_scaling': {'interleaved': True, 'mrope_section': [24, 20, 20], 'rope_type': 'default', 'type': 'default'}, 'attention_bias': False, 'attention_dropout': 0, 'code_predictor_config': {'return_dict': True, 'output_hidden_states': False, 'torchscript': False, 'dtype': None, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'architectures': None, 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'task_specific_params': None, 'problem_type': None, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': None, 'eos_token_id': None, 'sep_token_id': None, 'decoder_start_token_id': None, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'num_beam_groups': 1, 'diversity_penalty': 0.0, '_name_or_path': '', 'model_type': 'qwen3_tts_talker_code_predictor', 'tf_legacy_loss': False, 'use_bfloat16': False, 'vocab_size': 2048, 'max_position_embeddings': 65536, 'hidden_size': 1024, 'intermediate_size': 3072, 'num_hidden_layers': 5, 'num_attention_heads': 16, 'use_sliding_window': False, 'sliding_window': None, 'max_window_layers': 28, 'num_key_value_heads': 8, 'head_dim': 128, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-06, 'use_cache': True, 'rope_theta': 1000000, 'rope_scaling': None, 'attention_bias': False, 'attention_dropout': 0, 'layer_types': ['full_attention', 'full_attention', 'full_attention', 'full_attention', 'full_attention'], 'num_code_groups': 16, 'output_attentions': False}, 'num_code_groups': 16, 'text_hidden_size': 2048, 'codec_eos_token_id': 2150, 'codec_think_id': 2154, 'codec_language_id': {'chinese': 2055, 'english': 2050, 'german': 2053, 'italian': 2070, 'portuguese': 2071, 'spanish': 2054, 'japanese': 2058, 'korean': 2064, 'french': 2061, 'russian': 2069}, 'codec_nothink_id': 2155, 'codec_think_bos_id': 2156, 'codec_think_eos_id': 2157, 'codec_pad_id': 2148, 'codec_bos_id': 2149, 'spk_id': {}, 'spk_is_dialect': {}, 'output_attentions': False}\n",
      "speaker_encoder_config: {'mel_dim': 128, 'enc_dim': 1024, 'enc_channels': [512, 512, 512, 512, 1536], 'enc_kernel_sizes': [5, 3, 3, 3, 1], 'enc_dilations': [1, 2, 3, 4, 1], 'enc_attention_channels': 128, 'enc_res2net_scale': 8, 'enc_se_channels': 128, 'sample_rate': 24000, 'dtype': 'bfloat16', 'model_type': ''}\n",
      "tokenizer_type: qwen3_tts_tokenizer_12hz\n",
      "tts_model_size: 1b7\n",
      "tts_model_type: voice_design\n",
      "im_start_token_id: 151644\n",
      "im_end_token_id: 151645\n",
      "tts_pad_token_id: 151671\n",
      "tts_bos_token_id: 151672\n",
      "tts_eos_token_id: 151673\n",
      "output_attentions: False\n"
     ]
    }
   ],
   "source": [
    "# Backup original config\n",
    "from copy import deepcopy\n",
    "original_config = deepcopy(model.config)\n",
    "print(\"Original Model Config:\")\n",
    "for key, value in original_config.to_dict().items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92796a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOS token: None, ID: None\n",
      "EOS token: <|im_end|>, ID: 151645\n",
      "PAD token: <|endoftext|>, ID: 151643\n"
     ]
    }
   ],
   "source": [
    "print(f\"BOS token: {tokenizer.bos_token}, ID: {tokenizer.bos_token_id}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}, ID: {tokenizer.eos_token_id}\")\n",
    "print(f\"PAD token: {tokenizer.pad_token}, ID: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01fae88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151644\n"
     ]
    }
   ],
   "source": [
    "im_start_id = tokenizer.convert_tokens_to_ids(\"<|im_start|>\")\n",
    "print(im_start_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0af61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new config\n",
    "config = SelectiveTunerConfig.from_pretrained(\n",
    "    model.config,\n",
    "    anchor_token=anchor_token, anchor_token_id=anchor_token_id,\n",
    "    use_direct_anchor=use_direct_anchor, use_mixed_anchor=use_mixed_anchor, tie_embeddings=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebb05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_indices = torch.tensor(anchor_token_id)\n",
    "\n",
    "def make_mask_hook(indices, vocab_size):\n",
    "    mask = torch.zeros(vocab_size)\n",
    "    mask[indices] = 1.0\n",
    "    \n",
    "    def hook(grad):\n",
    "        m = mask.to(dtype=grad.dtype, device=grad.device)\n",
    "        return grad * m.unsqueeze(1)\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa3b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config setup\n",
    "if \"parler\" in model_id.lower():\n",
    "    setattr(config, 'hidden_size', config.decoder.hidden_size)  # parler-tts doesn't have decoder hidden_size conf\n",
    "elif \"qwen\" in model_id.lower():\n",
    "    setattr(config, 'vocab_size', config.talker_config.text_vocab_size)\n",
    "    setattr(config, 'hidden_size', config.talker_config.hidden_size)\n",
    "    model.talker.get_text_embeddings().weight.register_hook(\n",
    "        make_mask_hook(trainable_indices, config.talker_config.text_vocab_size)\n",
    "    )\n",
    "else:\n",
    "    pass\n",
    "\n",
    "config.hidden_size, config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fff9943384029a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply selective embedding tuner\n",
    "#SelectiveTunerForConditionalGeneration._replace_embeddings_with_anchors(model, config)\n",
    "#model.to(device=device, dtype=model_dtype)\n",
    "model.config = config  # override config\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36d9268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extend vocabulary\n",
    "SelectiveTunerForConditionalGeneration.extend_vocabulary(model, processor.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd30177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll-back to original config for saving\n",
    "model.config = original_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf76648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check model still works after modification\n",
    "if \"parler\" in model_id.lower():\n",
    "    prompt = \"Hey, how are you doing today?\"\n",
    "    #description = \"Jon's voice is monotone yet slightly fast in delivery, with a very close recording that almost has no background noise.\"\n",
    "    description = \"A female speaker delivers a slightly expressive and animated speech with a moderate speed and pitch. The recording is of very high quality, with the speaker's voice sounding clear and very close up.\"\n",
    "    input_ids, prompt_input_ids = (tokenizer(d, return_tensors=\"pt\").input_ids.to(device) for d in [description, prompt])\n",
    "\n",
    "    outputs = model.generate(input_ids=input_ids, prompt_input_ids=prompt_input_ids)\n",
    "    audio_values, sr = outputs.cpu().squeeze(), model.config.sampling_rate\n",
    "elif \"qwen\" in model_id.lower():\n",
    "    inputs = processor.encode_voice_design(\n",
    "        text=\"I am solving the equation: x = [-b ± √(b²-4ac)] / 2a? Nobody can — it's a disaster (◍•͈⌔•͈◍), very sad!\",\n",
    "        instruct=\"Happy man describes the equation in a cheerful tone, with a hint of humor. He emphasizes the complexity of the equation and expresses his feelings about it in a lighthearted way.\",\n",
    "    )\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "    audio_values, sr = processor.decode(outputs)\n",
    "    audio_values = torch.from_numpy(audio_values[0])\n",
    "else:\n",
    "    pass\n",
    "\n",
    "show_waveform(None, waveform=audio_values, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b36e791",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a9cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db097519",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = curated_dataset[50]\n",
    "sample_data_organized = dict(\n",
    "    instruction=sample_data['combined_prompt'],\n",
    "    text=sample_data['normalized_text'],\n",
    "    output=sample_data['waveform'],\n",
    "    sample_rate=sample_data['sample_rate']\n",
    ")\n",
    "sample_data_organized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch_list):\n",
    "    instructions = [item['combined_prompt'] for item in batch_list]\n",
    "    texts = [\n",
    "        (CONSISTENCY_TOKEN + item['normalized_text']) if item['distance_z_score'] < Z_THRESHOLD \n",
    "        else item['normalized_text'] \n",
    "        for item in batch_list\n",
    "    ]\n",
    "    outputs = [item['waveform'].cpu().numpy() for item in batch_list]\n",
    "    sample_rates = [item['sample_rate'] for item in batch_list]\n",
    "\n",
    "    # ── 1. audio → codec codes ──────────────────────────────────────────\n",
    "    prompt_codes = []  # list of (T, 16)\n",
    "    for wav, sr in zip(outputs, sample_rates):\n",
    "        audio_list = processor._normalize_audio_inputs([wav], sr)\n",
    "        feature_inputs = processor.feature_extractor(\n",
    "            raw_audio=audio_list,\n",
    "            sampling_rate=int(processor.feature_extractor.sampling_rate),\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device).to(processor.audio_tokenizer.dtype)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            audio_outputs = processor.audio_tokenizer.encode(\n",
    "                feature_inputs[\"input_values\"].squeeze(1),\n",
    "                feature_inputs[\"padding_mask\"].squeeze(1),\n",
    "                return_dict=True,\n",
    "            )\n",
    "        prompt_codes.append(audio_outputs.audio_codes[0])  # (C, 16)\n",
    "\n",
    "    # ── 2. text tokenize ─────────────────────────────────────────────────\n",
    "    text_id_list = []\n",
    "    instruct_id_list = []\n",
    "    for text, instruct in zip(texts, instructions):\n",
    "        t_ids = tokenizer(\n",
    "            f\"<|im_start|>assistant\\n{text}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids  # (1, L)\n",
    "        i_ids = tokenizer(\n",
    "            f\"<|im_start|>user\\n{instruct}<|im_end|>\\n\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids  # (1, Li)\n",
    "        text_id_list.append(t_ids)\n",
    "        instruct_id_list.append(i_ids)\n",
    "\n",
    "    # ── 3. embedding 계산 및 context 구성 ───────────────────────────────\n",
    "    talker = model.talker\n",
    "    cfg = model.config\n",
    "\n",
    "    def text_embed(ids):\n",
    "        return talker.text_projection(talker.get_text_embeddings()(ids.to(device)))\n",
    "\n",
    "    def codec_embed_0(ids):\n",
    "        return talker.get_input_embeddings()(ids.to(device))\n",
    "\n",
    "    sample_embeds = []\n",
    "    codec_0_labels_list = []\n",
    "    codec_ids_list = []\n",
    "    codec_mask_list = []\n",
    "\n",
    "    for t_ids, ins_ids, codes in zip(text_id_list, instruct_id_list, prompt_codes):\n",
    "        # t_ids: (1, L),  codes: (C, 16)\n",
    "        C = codes.shape[0]\n",
    "        text_body_len = t_ids.shape[1] - 3 - 5  # role(3) + trailing(5) 제외\n",
    "\n",
    "        tts_bos_embed, tts_eos_embed, tts_pad_embed = text_embed(\n",
    "            torch.tensor([[cfg.tts_bos_token_id, cfg.tts_eos_token_id, cfg.tts_pad_token_id]])\n",
    "        ).chunk(3, dim=1)  # 각 (1, 1, D)\n",
    "\n",
    "        # ① instruct: text channel만\n",
    "        e_instruct = text_embed(ins_ids)  # (1, Li, D)\n",
    "\n",
    "        # ② role: \"<|im_start|>assistant\\n\" (3 tokens)\n",
    "        e_role = text_embed(t_ids[:, :3])  # (1, 3, D)\n",
    "\n",
    "        # ③ codec prefix(4칸): text=[tts_pad*3, tts_bos], codec=[nothink, think_bos, think_eos, codec_pad]\n",
    "        e_codec_prefix = codec_embed_0(\n",
    "            torch.tensor([[\n",
    "                cfg.talker_config.codec_nothink_id,\n",
    "                cfg.talker_config.codec_think_bos_id,\n",
    "                cfg.talker_config.codec_think_eos_id,\n",
    "                cfg.talker_config.codec_pad_id,\n",
    "            ]])\n",
    "        )  # (1, 4, D)\n",
    "        e_text_prefix = torch.cat([\n",
    "            tts_pad_embed.expand(-1, 3, -1),\n",
    "            tts_bos_embed,\n",
    "        ], dim=1)  # (1, 4, D)\n",
    "        e_prefix = e_text_prefix + e_codec_prefix  # (1, 4, D)\n",
    "\n",
    "        # ④ text body + tts_eos / codec=codec_pad\n",
    "        e_text_body = torch.cat([\n",
    "            text_embed(t_ids[:, 3:-5]),  # (1, text_body_len, D)\n",
    "            tts_eos_embed,               # (1, 1, D)\n",
    "        ], dim=1)  # (1, text_body_len+1, D)\n",
    "        e_codec_pad_body = codec_embed_0(\n",
    "            torch.tensor([[cfg.talker_config.codec_pad_id] * (text_body_len + 1)])\n",
    "        )  # (1, text_body_len+1, D)\n",
    "        e_body = e_text_body + e_codec_pad_body  # (1, text_body_len+1, D)\n",
    "\n",
    "        # ⑤ codec_bos: text=tts_pad, codec=codec_bos\n",
    "        e_codec_bos = tts_pad_embed + codec_embed_0(\n",
    "            torch.tensor([[cfg.talker_config.codec_bos_id]])\n",
    "        )  # (1, 1, D)\n",
    "\n",
    "        # ⑥ audio codec tokens: text=tts_pad, codec=sum of all quantizers\n",
    "        e_audio = tts_pad_embed.expand(-1, C, -1).clone()\n",
    "        e_audio = e_audio + codec_embed_0(codes[:, 0].unsqueeze(0))  # codebook 0\n",
    "        for q in range(1, 16):\n",
    "            e_audio = e_audio + talker.code_predictor.get_input_embeddings()[q-1](\n",
    "                codes[:, q].unsqueeze(0)\n",
    "            )  # (1, C, D)\n",
    "\n",
    "        # ⑦ codec EOS: text=tts_pad, codec=codec_eos\n",
    "        e_codec_eos = tts_pad_embed + codec_embed_0(\n",
    "            torch.tensor([[cfg.talker_config.codec_eos_token_id]])\n",
    "        )  # (1, 1, D)\n",
    "\n",
    "        # 전체 concat\n",
    "        full_embed = torch.cat([\n",
    "            e_instruct,   # (1, Li, D)\n",
    "            e_role,       # (1, 3, D)\n",
    "            e_prefix,     # (1, 4, D)\n",
    "            e_body,       # (1, text_body_len+1, D)\n",
    "            e_codec_bos,  # (1, 1, D)\n",
    "            e_audio,      # (1, C, D)\n",
    "            e_codec_eos,  # (1, 1, D)\n",
    "        ], dim=1)  # (1, T, D)\n",
    "        sample_embeds.append(full_embed)\n",
    "\n",
    "        # labels: codec 구간 + EOS만 살리고 나머지 -100\n",
    "        total_len = full_embed.shape[1]\n",
    "        prefix_len = ins_ids.shape[1] + 3 + 4 + (text_body_len + 1) + 1  # codec token 시작 위치\n",
    "        labels = torch.full((total_len,), -100, dtype=torch.long)\n",
    "        labels[prefix_len: prefix_len + C] = codes[:, 0]\n",
    "        labels[prefix_len + C] = cfg.talker_config.codec_eos_token_id\n",
    "        codec_0_labels_list.append(labels)\n",
    "\n",
    "        # codec_ids: (total_len, 16)\n",
    "        cids = torch.zeros(total_len, 16, dtype=torch.long)\n",
    "        cids[prefix_len: prefix_len + C] = codes\n",
    "        codec_ids_list.append(cids)\n",
    "\n",
    "        # codec_mask: audio codec 구간만 True (EOS 제외 — sft_12hz.py 동일)\n",
    "        cmask = torch.zeros(total_len, dtype=torch.bool)\n",
    "        cmask[prefix_len: prefix_len + C] = True\n",
    "        codec_mask_list.append(cmask)\n",
    "\n",
    "    # ── 4. left-padding & batch ──────────────────────────────────────────\n",
    "    B = len(sample_embeds)\n",
    "    seqs = [e.squeeze(0) for e in sample_embeds]\n",
    "    original_lengths = torch.tensor([s.shape[0] for s in seqs])\n",
    "    max_len = original_lengths.max().item()\n",
    "\n",
    "    # inputs_embeds: left pad\n",
    "    seqs_rev = [s.flip(0) for s in seqs]\n",
    "    padded_rev = torch.nn.utils.rnn.pad_sequence(seqs_rev, batch_first=True, padding_value=0.0)\n",
    "    inputs_embeds = padded_rev.flip(1).to(device)  # (B, max_len, D)\n",
    "\n",
    "    # attention_mask\n",
    "    num_pads = max_len - original_lengths\n",
    "    indices = torch.arange(max_len).expand(B, -1)\n",
    "    attention_mask = (indices >= num_pads.unsqueeze(1)).long().to(device)\n",
    "\n",
    "    # labels, codec_ids, codec_mask: left pad\n",
    "    codec_0_labels = torch.full((B, max_len), -100, dtype=torch.long)\n",
    "    codec_ids      = torch.zeros(B, max_len, 16, dtype=torch.long)\n",
    "    codec_mask     = torch.zeros(B, max_len, dtype=torch.bool)\n",
    "\n",
    "    for i, (lab, cids, cmask, l) in enumerate(zip(codec_0_labels_list, codec_ids_list, codec_mask_list, original_lengths)):\n",
    "        codec_0_labels[i, max_len - l:] = lab\n",
    "        codec_ids[i, max_len - l:]      = cids\n",
    "        codec_mask[i, max_len - l:]     = cmask\n",
    "\n",
    "    codec_0_labels = codec_0_labels.to(device)\n",
    "    codec_ids      = codec_ids.to(device)\n",
    "    codec_mask     = codec_mask.to(device)\n",
    "\n",
    "    return dict(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=attention_mask,\n",
    "        codec_0_labels=codec_0_labels,\n",
    "        codec_ids=codec_ids,\n",
    "        codec_mask=codec_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(curated_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=12, collate_fn=collate_fn)\n",
    "data_loader = DataLoader(curated_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375568c6ddc2059",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9c6df",
   "metadata": {},
   "source": [
    "### Set trainable params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25823001cc14ce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for module in model.modules():\n",
    "    if isinstance(module, (DirectStyleAnchorEmbedding, EncoderStyleAnchorEmbedding, MixedStyleAnchorEmbedding)):\n",
    "        print(f\"INFO: Found a target embedding instance: {type(module).__name__}\")\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if hasattr(module, 'q_proj') and hasattr(module, 'k_proj'):\n",
    "        print(f\"INFO: Unfreezing Q and K projections in: {type(module).__name__}\")\n",
    "        module.q_proj.weight.requires_grad = True\n",
    "        module.k_proj.weight.requires_grad = True\n",
    "        if module.q_proj.bias is not None:\n",
    "            module.q_proj.bias.requires_grad = True\n",
    "        if module.k_proj.bias is not None:\n",
    "            module.k_proj.bias.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaee9c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"qwen\" in model_id.lower():\n",
    "    print(\"INFO: Unfreezing token embeddings in talker...\")\n",
    "    model.talker.get_text_embeddings().weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73df2621",
   "metadata": {},
   "source": [
    "### Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cafc886d53d38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "OUTPUT_DIR = \"./results/\" + model_id + \"/\" + result_id\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb09013cf19fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=LEARNING_RATE, weight_decay=0.01)\n",
    "epoch_steps = int(len(curated_dataset) / BATCH_SIZE + 0.99)\n",
    "total_steps = epoch_steps * NUM_EPOCHS\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=LEARNING_RATE/100)\n",
    "\n",
    "print(f\"INFO: dataset len={len(curated_dataset)}, total_steps={total_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e7713",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LinearLR, SequentialLR\n",
    "\n",
    "# 파라미터 그룹 분리\n",
    "embed_params = [model.talker.get_text_embeddings().weight]\n",
    "qk_params = [\n",
    "    p for name, p in model.named_parameters()\n",
    "    if p.requires_grad and 'embed' not in name\n",
    "]\n",
    "\n",
    "optimizer = AdamW([\n",
    "    {\"params\": embed_params, \"lr\": 1e-3},\n",
    "    {\"params\": qk_params,    \"lr\": 2e-5},\n",
    "], weight_decay=0.01)\n",
    "\n",
    "# Scheduler\n",
    "warmup_steps = 100\n",
    "cosine_steps = total_steps - warmup_steps\n",
    "\n",
    "warmup = LinearLR(optimizer, start_factor=0.01, end_factor=1.0, total_iters=warmup_steps)\n",
    "cosine = CosineAnnealingLR(optimizer, T_max=cosine_steps, eta_min=2e-6)  # embed eta_min도 2e-4로 따로 쓰고 싶으면 param group별 scheduler 분리 필요\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup, cosine], milestones=[warmup_steps])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7a0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_EPOCH = 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2341622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dtype(config):\n",
    "    config.__dict__.pop('dtype', None)\n",
    "    for v in config.__dict__.values():\n",
    "        from transformers import PretrainedConfig\n",
    "        if isinstance(v, PretrainedConfig):\n",
    "            remove_dtype(v)\n",
    "\n",
    "remove_dtype(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbbdc141ae2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "\n",
    "total_bar = NotebookProgressBar(NUM_EPOCHS, prefix=\"Running Epochs\")\n",
    "for epoch in range(START_EPOCH-1, NUM_EPOCHS+60):\n",
    "    total_bar.update(epoch+1)\n",
    "    train_loss, train_mfcc = [], []\n",
    "\n",
    "    train_bar = NotebookProgressBar(epoch_steps, prefix=f\"Training {epoch+1}\")\n",
    "    for i, inputs in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        try:\n",
    "            if \"qwen\" in model_id.lower():\n",
    "                outputs = model.talker(\n",
    "                    inputs_embeds=inputs['inputs_embeds'][:, :-1, :],\n",
    "                    attention_mask=inputs['attention_mask'][:, :-1],\n",
    "                    labels=inputs['codec_0_labels'][:, 1:],\n",
    "                    output_hidden_states=True\n",
    "                )\n",
    "\n",
    "                hidden_states = outputs.hidden_states[0][-1]\n",
    "\n",
    "                # codec_mask도 :-1 로 잘라서 inputs_embeds와 길이 맞추기\n",
    "                codec_mask_shifted = inputs['codec_mask'][:, :-1]\n",
    "                talker_hidden_states = hidden_states[codec_mask_shifted]\n",
    "                talker_codec_ids = inputs['codec_ids'][:, :-1][codec_mask_shifted]\n",
    "\n",
    "                sub_talker_logits, sub_talker_loss = model.talker.forward_sub_talker_finetune(\n",
    "                    talker_codec_ids, talker_hidden_states\n",
    "                )\n",
    "                outputs.loss = outputs.loss + 0.3 * sub_talker_loss\n",
    "            else:\n",
    "                outputs = model(**inputs)\n",
    "        except (torch.cuda.OutOfMemoryError, RuntimeError):\n",
    "            import gc\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            continue\n",
    "\n",
    "        losses = outputs.loss\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss.append(losses.item())\n",
    "\n",
    "        if i+1 != train_bar.total: train_bar.update(i+1, comment=f\"Loss={losses.item():.5f}, LR={optimizer.param_groups[0]['lr']:.1e}\")\n",
    "\n",
    "    model.save_pretrained(OUTPUT_DIR+f\"_epoch{epoch+1}\")\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    train_bar.update(train_bar.total, comment=f\"Loss={sum(train_loss)/len(train_loss):.5f}, LR={optimizer.param_groups[0]['lr']:.1e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(OUTPUT_DIR+\"_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40018af7",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spk_incon.metrics.presets import DatasetType, GenerationMethod, SynthesisConfig, ModelType\n",
    "from spk_incon.metrics.strategies import create_strategy\n",
    "from spk_incon.datasets import DatasetType, create_dataset\n",
    "\n",
    "from spk_incon.utils.evaluate import EvaluationPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f57fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_config = SynthesisConfig()\n",
    "test_dataset_type = DatasetType.LIBRITTS\n",
    "test_dataset_config = test_config.get_dataset_config(test_dataset_type.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e0a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = create_dataset(test_dataset_type, test_dataset_config, root_dir=\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6c2fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "class TestModel:\n",
    "    @classmethod\n",
    "    def seed_everything(cls, seed: int = 42):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    @classmethod\n",
    "    def synthesize(\n",
    "        cls,\n",
    "        text: str,\n",
    "        output_path: Path,\n",
    "        reference_audio: Path | None = None,\n",
    "        style_prompt: str | None = None,\n",
    "        speaker_id: str | None = None\n",
    "    ) -> bool:\n",
    "        cls.seed_everything()\n",
    "        output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Adjust style prompt\n",
    "        if ADD_CONSISTENCY_TOKEN:\n",
    "            text = f\"{CONSISTENCY_TOKEN}{text}\"\n",
    "        if ADD_STYLE_TOKEN:\n",
    "            style_prompt = f\"{style_prompt}{STYLE_TOKEN}\"\n",
    "\n",
    "        # Setup generation config\n",
    "        generation_config = dict(\n",
    "            #top_k=1,\n",
    "        )\n",
    "\n",
    "        # Input preparation\n",
    "        if \"parler\" in model_id.lower():\n",
    "            inputs = dict(\n",
    "                input_ids=tokenizer(style_prompt, return_tensors=\"pt\").input_ids.to(device),\n",
    "                prompt_input_ids=tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "            )\n",
    "        elif \"qwen\" in model_id.lower():\n",
    "            inputs = processor.encode_voice_design(\n",
    "                text=text, instruct=style_prompt,\n",
    "            )\n",
    "\n",
    "        # Generation\n",
    "        outputs = model.generate(**inputs, **generation_config)\n",
    "\n",
    "        # Decoding\n",
    "        if \"parler\" in model_id.lower():\n",
    "            audio_values = outputs.cpu().numpy().squeeze()\n",
    "            sample_rate = config.audio_encoder.sampling_rate\n",
    "        elif \"qwen\" in model_id.lower():\n",
    "            audio_values, sample_rate = processor.decode(outputs)\n",
    "            audio_values = audio_values[0]\n",
    "\n",
    "        # Save audio\n",
    "        sf.write(output_path, audio_values, sample_rate)\n",
    "        try:\n",
    "            return output_path.stat().st_size > 0\n",
    "        except FileNotFoundError:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3e9c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class ModelType(Enum):\n",
    "    TEST = model.__class__.__name__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2e94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "test_model_type = ModelType.TEST\n",
    "test_model = TestModel()\n",
    "\n",
    "try:\n",
    "    evaluator = EvaluationPipeline(base_dir=Path(OUTPUT_DIR+f\"_epoch{epoch+1}\"))\n",
    "    test_config.generation.output_dir = Path(OUTPUT_DIR+f\"_epoch{epoch+1}\")\n",
    "except UnboundLocalError:\n",
    "    evaluator = EvaluationPipeline(base_dir=Path(OUTPUT_DIR+\"_last\"))\n",
    "    test_config.generation.output_dir = Path(OUTPUT_DIR+\"_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917882d7",
   "metadata": {},
   "source": [
    "### Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024ca4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = create_strategy(GenerationMethod.METHOD2, test_config, test_dataset, test_model)\n",
    "exp2_result = strategy.generate_all(test_dataset_type.value, test_model_type.value)\n",
    "exp2_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb1866",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp2_eval_result = evaluator.evaluate_dataset_model(\n",
    "    dataset_type=test_dataset_type,\n",
    "    model_type=test_model_type,\n",
    "    methods=[GenerationMethod.METHOD2]\n",
    ")\n",
    "evaluator.save_results_to_csv(exp2_eval_result, test_dataset_type, test_model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e996005",
   "metadata": {},
   "source": [
    "### Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad38bedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = create_strategy(GenerationMethod.METHOD1, test_config, test_dataset, test_model)\n",
    "exp1_result = strategy.generate_all(test_dataset_type.value, test_model_type.value)\n",
    "exp1_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b9807c",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_eval_result = evaluator.evaluate_dataset_model(\n",
    "    dataset_type=test_dataset_type,\n",
    "    model_type=test_model_type,\n",
    "    methods=[GenerationMethod.METHOD1]\n",
    ")\n",
    "evaluator.save_results_to_csv(exp1_eval_result, test_dataset_type, test_model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43016740",
   "metadata": {},
   "source": [
    "### Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e83ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = create_strategy(GenerationMethod.METHOD3, test_config, test_dataset, test_model)\n",
    "exp3_result = strategy.generate_all(test_dataset_type.value, test_model_type.value)\n",
    "exp3_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1f36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp3_eval_result = evaluator.evaluate_dataset_model(\n",
    "    dataset_type=test_dataset_type,\n",
    "    model_type=test_model_type,\n",
    "    methods=[GenerationMethod.METHOD3]\n",
    ")\n",
    "evaluator.save_results_to_csv(exp3_eval_result, test_dataset_type, test_model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0136bc33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spk-incon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
